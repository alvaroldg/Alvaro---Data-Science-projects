{"cells":[{"cell_type":"code","execution_count":null,"id":"2c4e6375","metadata":{"id":"2c4e6375","outputId":"7886c338-1ed3-44e4-a7fe-38a04b59fa9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting Torch==1.8.1\n","  Using cached torch-1.8.1-cp39-cp39-win_amd64.whl (190.5 MB)\n","Requirement already satisfied: typing-extensions in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from Torch==1.8.1) (4.3.0)\n","Requirement already satisfied: numpy in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from Torch==1.8.1) (1.21.5)\n","Installing collected packages: Torch\n","  Attempting uninstall: Torch\n","    Found existing installation: torch 2.0.1\n","    Uninstalling torch-2.0.1:\n","      Successfully uninstalled torch-2.0.1\n","Successfully installed Torch-1.8.1\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.15.2 requires torch==2.0.1, but you have torch 1.8.1 which is incompatible.\n","\n","[notice] A new release of pip available: 22.3.1 -> 23.1.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: TorchVision in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (0.15.2)\n","Requirement already satisfied: requests in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from TorchVision) (2.28.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from TorchVision) (9.2.0)\n","Collecting torch==2.0.1\n","  Using cached torch-2.0.1-cp39-cp39-win_amd64.whl (172.4 MB)\n","Requirement already satisfied: numpy in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from TorchVision) (1.21.5)\n","Requirement already satisfied: filelock in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from torch==2.0.1->TorchVision) (3.6.0)\n","Requirement already satisfied: sympy in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from torch==2.0.1->TorchVision) (1.10.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from torch==2.0.1->TorchVision) (4.3.0)\n","Requirement already satisfied: networkx in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from torch==2.0.1->TorchVision) (2.8.4)\n","Requirement already satisfied: jinja2 in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from torch==2.0.1->TorchVision) (2.11.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from requests->TorchVision) (1.26.11)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from requests->TorchVision) (3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from requests->TorchVision) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from requests->TorchVision) (2.0.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.1->TorchVision) (2.0.1)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.1->TorchVision) (1.2.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.8.1\n","    Uninstalling torch-1.8.1:\n","      Successfully uninstalled torch-1.8.1\n","Successfully installed torch-2.0.1\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.3.1 -> 23.1.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["#INSTALACIÓN DE LIBRERIAS\n","!pip install Torch==1.8.1\n","!pip install TorchVision"]},{"cell_type":"code","execution_count":null,"id":"dad83057","metadata":{"id":"dad83057","outputId":"5224de6a-8702-4eec-999a-dd7fccb8a34c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opencv-python in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (4.7.0.72)\n","Requirement already satisfied: numpy>=1.17.3 in c:\\users\\alvaroldg\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.3.1 -> 23.1.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install opencv-python"]},{"cell_type":"code","execution_count":null,"id":"605c0e3f","metadata":{"id":"605c0e3f"},"outputs":[],"source":["import os\n","import shutil\n","import cv2"]},{"cell_type":"code","execution_count":null,"id":"8f7b7610","metadata":{"id":"8f7b7610"},"outputs":[],"source":["# Ruta de la carpeta que contiene las imágenes\n","ruta_carpeta = r'D:\\Master\\TFM\\IDC_regular_ps50_idx5\\00Prueba2'\n","\n","# Lista para almacenar los arrays de las imágenes\n","listado_arrays_imagenes = []\n","listado_arrays_etiquetas = []\n","\n","# Recorrer las subcarpetas '0' y '1'\n","for subcarpeta in ['0', '1']:\n","    subcarpeta_ruta = ruta_carpeta +\"\\\\\" + subcarpeta\n","\n","    # Para cada archivo en la subcarpeta\n","    for nombre_archivo in os.listdir(subcarpeta_ruta):\n","        # Comprobar si el archivo es una imagen (esto dependerá de las extensiones de archivo de tus imágenes)\n","        if nombre_archivo.endswith('.png'):\n","            # Construir la ruta completa del archivo\n","            ruta_archivo = os.path.join(subcarpeta_ruta, nombre_archivo)\n","\n","            # Leer la imagen y convertirla a un array\n","            img = cv2.imread(ruta_archivo)\n","            # Añadir el array a la lista\n","            listado_arrays_imagenes.append(img)\n","            # Añadir la etiqueta correspondiente a la lista de etiquetas\n","            listado_arrays_etiquetas.append(int(subcarpeta))"]},{"cell_type":"code","execution_count":null,"id":"a4379ee9","metadata":{"id":"a4379ee9"},"outputs":[],"source":["import numpy as np\n","\n","# Convierte listado_arrays_etiquetas a numpy array\n","etiquetas = np.array(listado_arrays_etiquetas)\n","\n","# Obtén los índices de las imágenes con etiqueta 0 y selecciona los primeros 5000\n","indices_0 = np.where(etiquetas == 0)[0][:500]\n","\n","# Haz lo mismo para las imágenes con etiqueta 1\n","indices_1 = np.where(etiquetas == 1)[0][:500]\n","\n","# Concatena los índices de las imágenes de ambas etiquetas\n","indices = np.concatenate([indices_0, indices_1])\n","\n","# Utiliza los índices para seleccionar las imágenes y etiquetas correspondientes\n","listado_arrays_imagenes = [listado_arrays_imagenes[i] for i in indices]\n","listado_arrays_etiquetas = [listado_arrays_etiquetas[i] for i in indices]\n"]},{"cell_type":"code","execution_count":null,"id":"92eafbd5","metadata":{"id":"92eafbd5"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Asumimos que la lista 'listado_arrays_imagenes' existe y contiene las matrices de imágenes\n","\n","# Verificar la forma de las matrices\n","shapes = set(img.shape for img in listado_arrays_imagenes)\n","if len(shapes) > 1:\n","    # Realizar operación de ajuste en las matrices para que tengan la misma forma\n","    max_shape = max(shapes, key=lambda x: np.prod(x))\n","    listado_arrays_imagenes = [np.pad(img, [(0, max_shape[0]-img.shape[0]),\n","                                           (0, max_shape[1]-img.shape[1]),\n","                                           (0, max_shape[2]-img.shape[2])],\n","                                      mode='constant', constant_values=0)\n","                               for img in listado_arrays_imagenes]\n","\n","# Convertir la lista de imágenes en una matriz 4D\n","imagenes = np.stack(listado_arrays_imagenes)\n","\n","# Obtener las dimensiones de las imágenes\n","num_imagenes, altura, anchura, canales = imagenes.shape\n","\n","# Reshape para que las imágenes sean 2D\n","imagenes_2d = imagenes.reshape(num_imagenes, -1)\n","\n","# Crear una instancia del escalador MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","# Aplicar el escalado a las imágenes\n","imagenes_escaladas = scaler.fit_transform(imagenes_2d)\n","\n","# Reshape de nuevo a las dimensiones originales de las imágenes\n","imagenes_escaladas = imagenes_escaladas.reshape(num_imagenes, altura, anchura, canales)\n"]},{"cell_type":"code","execution_count":null,"id":"0bb42b30","metadata":{"id":"0bb42b30","outputId":"bcea7144-2781-4afe-86d7-84baccb63495"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cantidad de etiquetas 1: 500\n","Cantidad de etiquetas 0: 500\n"]}],"source":["# Contar las etiquetas 1\n","count_1 = listado_arrays_etiquetas.count(1)\n","print(f'Cantidad de etiquetas 1: {count_1}')\n","\n","# Contar las etiquetas 0\n","count_0 = listado_arrays_etiquetas.count(0)\n","print(f'Cantidad de etiquetas 0: {count_0}')"]},{"cell_type":"code","execution_count":null,"id":"76abd2d3","metadata":{"id":"76abd2d3","outputId":"e23bd640-38fc-4e87-ea99-e1f3c21b6567"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 10 neurons\n","CREAR MODELO 10\n","Loaded checkpoint C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0010_10.pth\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0011_10.pth\n","Epoch: 11, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0012_10.pth\n","Epoch: 12, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0013_10.pth\n","Epoch: 13, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0014_10.pth\n","Epoch: 14, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0015_10.pth\n","Epoch: 15, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0016_10.pth\n","Epoch: 16, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0017_10.pth\n","Epoch: 17, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0018_10.pth\n","Epoch: 18, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0019_10.pth\n","Epoch: 19, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0020_10.pth\n","Epoch: 20, Accuracy: 0.52\n","Using 20 neurons\n","CREAR MODELO 20\n","Loaded checkpoint C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0010_20.pth\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0011_20.pth\n","Epoch: 11, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0012_20.pth\n","Epoch: 12, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0013_20.pth\n","Epoch: 13, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0014_20.pth\n","Epoch: 14, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0015_20.pth\n","Epoch: 15, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0016_20.pth\n","Epoch: 16, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0017_20.pth\n","Epoch: 17, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0018_20.pth\n","Epoch: 18, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0019_20.pth\n","Epoch: 19, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0020_20.pth\n","Epoch: 20, Accuracy: 0.52\n","Using 30 neurons\n","CREAR MODELO 30\n","Loaded checkpoint C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0010_30.pth\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0011_30.pth\n","Epoch: 11, Accuracy: 0.48\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0012_30.pth\n","Epoch: 12, Accuracy: 0.48\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0013_30.pth\n","Epoch: 13, Accuracy: 0.48\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0014_30.pth\n","Epoch: 14, Accuracy: 0.48\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0015_30.pth\n","Epoch: 15, Accuracy: 0.48\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0016_30.pth\n","Epoch: 16, Accuracy: 0.48\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0017_30.pth\n","Epoch: 17, Accuracy: 0.48\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0018_30.pth\n","Epoch: 18, Accuracy: 0.48\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0019_30.pth\n","Epoch: 19, Accuracy: 0.48\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0020_30.pth\n","Epoch: 20, Accuracy: 0.48\n","Using 40 neurons\n","CREAR MODELO 40\n","Loaded checkpoint C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0010_40.pth\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0011_40.pth\n","Epoch: 11, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0012_40.pth\n","Epoch: 12, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0013_40.pth\n","Epoch: 13, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0014_40.pth\n","Epoch: 14, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0015_40.pth\n","Epoch: 15, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0016_40.pth\n","Epoch: 16, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0017_40.pth\n","Epoch: 17, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0018_40.pth\n","Epoch: 18, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0019_40.pth\n","Epoch: 19, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0020_40.pth\n","Epoch: 20, Accuracy: 0.52\n","Using 50 neurons\n","CREAR MODELO 50\n","Loaded checkpoint C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0010_50.pth\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0011_50.pth\n","Epoch: 11, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0012_50.pth\n","Epoch: 12, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0013_50.pth\n","Epoch: 13, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0014_50.pth\n","Epoch: 14, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0015_50.pth\n","Epoch: 15, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0016_50.pth\n","Epoch: 16, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0017_50.pth\n","Epoch: 17, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0018_50.pth\n","Epoch: 18, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0019_50.pth\n","Epoch: 19, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0020_50.pth\n","Epoch: 20, Accuracy: 0.52\n","Using 100 neurons\n","CREAR MODELO 100\n","Loaded checkpoint C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0010_100.pth\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0011_100.pth\n","Epoch: 11, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0012_100.pth\n","Epoch: 12, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0013_100.pth\n","Epoch: 13, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0014_100.pth\n","Epoch: 14, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0015_100.pth\n","Epoch: 15, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0016_100.pth\n","Epoch: 16, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0017_100.pth\n","Epoch: 17, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0018_100.pth\n","Epoch: 18, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0019_100.pth\n","Epoch: 19, Accuracy: 0.52\n","Saved checkpoint into C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0020_100.pth\n","Epoch: 20, Accuracy: 0.52\n","Best Accuracy: 0.52\n","Best Parameters: {'num_neurons': 10}\n"]}],"source":["# Import necessary libraries\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import ParameterGrid\n","\n","# Convert lists to tensors\n","imagenes = torch.stack([torch.Tensor(i) for i in imagenes])\n","etiquetas = torch.tensor(listado_arrays_etiquetas)\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(imagenes, etiquetas, test_size=0.2, random_state=42)\n","\n","# Define custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# Create train and test datasets\n","train_dataset = CustomDataset(X_train, y_train)\n","test_dataset = CustomDataset(X_test, y_test)\n","\n","# Define the model architecture\n","class Model(nn.Module):\n","    def __init__(self, num_neurons):\n","        super(Model, self).__init__()\n","        self.conv1 = nn.Conv2d(3, num_neurons, 3)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc = nn.Linear(num_neurons*24*24, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = x.reshape(-1, self.num_flat_features(x))\n","        x = self.sigmoid(self.fc(x))\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","\n","# Define the function to create the model\n","def crear_modelo(num_neurons):\n","    print(\"CREAR MODELO\", num_neurons)\n","    model = Model(num_neurons)\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters())\n","    return model, criterion, optimizer\n","\n","# Define the checkpoint directory\n","checkpoint_dir = 'D:\\Master\\TFM\\Checkpoints'\n","\n","# Create checkpoint directory if it doesn't exist\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","def save_ckpt(model, optimizer, epoch, num_neurons, checkpoint_dir):\n","    save_dir = os.path.join(checkpoint_dir, \"checkpoint_{:04d}_{}.pth\".format(epoch, num_neurons))\n","\n","    cp = {\n","        'model': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","        'epoch': epoch,\n","    }\n","\n","    torch.save(cp, save_dir)\n","    print(\"Saved checkpoint into {}\".format(save_dir))\n","\n","\n","def load_ckpt(checkpoint_dir, num_neurons):\n","    last_epoch = 0\n","    checkpoint_file = ''\n","\n","    for file in os.listdir(checkpoint_dir):\n","        if file.endswith('{}.pth'.format(num_neurons)):\n","            epoch_number = int(file.split('_')[1])\n","            if epoch_number > last_epoch:\n","                last_epoch = epoch_number\n","                checkpoint_file = file\n","\n","    if last_epoch == 0:\n","        return None, 0\n","    else:\n","        print(\"Loaded checkpoint\", os.path.join(checkpoint_dir, checkpoint_file))\n","        checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoint_file))\n","        return checkpoint, checkpoint['epoch']\n","\n","# Define the grid of parameters\n","param_grid = {\n","    'num_neurons': [10, 20, 30]\n","}\n","\n","# Perform grid search\n","best_accuracy = 0.0\n","best_params = None\n","\n","for params in ParameterGrid(param_grid):\n","    print(\"Using {} neurons\".format(params['num_neurons']))\n","    model, criterion, optimizer = crear_modelo(params['num_neurons'])\n","    checkpoint, start_epoch = load_ckpt(checkpoint_dir, params['num_neurons'])\n","    if checkpoint is not None:\n","        model.load_state_dict(checkpoint['model'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n","\n","    for epoch in range(start_epoch, start_epoch + 10):\n","        model.train()\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            inputs = inputs.permute((0,3,1,2))\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels.float().unsqueeze(1))\n","            loss.backward()\n","            optimizer.step()\n","\n","        save_ckpt(model, optimizer, epoch + 1, params['num_neurons'], checkpoint_dir) # Save the checkpoint\n","\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in test_loader:\n","                inputs = inputs.permute((0,3,1,2))\n","                outputs = model(inputs)\n","                predicted = (outputs >= 0.5).squeeze().long()\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        accuracy = correct / total\n","        print(\"Epoch: {}, Accuracy: {}\".format(epoch + 1, accuracy))\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_params = params\n","\n","print(\"Best Accuracy:\", best_accuracy)\n","print(\"Best Parameters:\", best_params)"]},{"cell_type":"code","execution_count":null,"id":"6481154c","metadata":{"id":"6481154c","outputId":"6cc38aca-9cd4-410c-8877-28c869980b3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["The prediction for image 12821_idx5_x1001_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1001_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1001_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1001_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1001_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1051_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1051_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1051_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1051_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1051_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1051_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1051_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1051_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1101_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1151_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1151_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1151_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1151_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1151_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1151_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1151_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1151_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1151_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1201_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1201_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1201_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1201_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1201_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1201_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1201_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1201_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1201_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1251_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1251_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1251_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1251_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1251_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1251_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1251_y1851_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1851_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1901_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y1951_class1.png is: 1\n","The prediction for image 12821_idx5_x1301_y2001_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1851_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1901_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y1951_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y2001_class1.png is: 1\n","The prediction for image 12821_idx5_x1351_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1851_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1901_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y1951_class1.png is: 1\n","The prediction for image 12821_idx5_x1401_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y1851_class1.png is: 1\n","The prediction for image 12821_idx5_x1451_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y1851_class1.png is: 1\n","The prediction for image 12821_idx5_x1501_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y851_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y901_class1.png is: 1\n","The prediction for image 12821_idx5_x1551_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y1801_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y851_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y901_class1.png is: 1\n","The prediction for image 12821_idx5_x1601_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y851_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y901_class1.png is: 1\n","The prediction for image 12821_idx5_x1651_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y851_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y901_class1.png is: 1\n","The prediction for image 12821_idx5_x1701_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y1751_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y851_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y901_class1.png is: 1\n","The prediction for image 12821_idx5_x1751_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y801_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y851_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y901_class1.png is: 1\n","The prediction for image 12821_idx5_x1801_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y801_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y851_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y901_class1.png is: 1\n","The prediction for image 12821_idx5_x1851_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y751_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y801_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y851_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y901_class1.png is: 1\n","The prediction for image 12821_idx5_x1901_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y1651_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y751_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y801_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y851_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y901_class1.png is: 1\n","The prediction for image 12821_idx5_x1951_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1001_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y1601_class1.png is: 1\n","The prediction for image 12821_idx5_x2001_y951_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1051_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1101_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1451_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1501_class1.png is: 1\n","The prediction for image 12821_idx5_x2051_y1551_class1.png is: 1\n","The prediction for image 12821_idx5_x2101_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x2101_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x2101_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x2101_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x2101_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x2101_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x2151_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x2151_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x2151_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x2151_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x2151_y1401_class1.png is: 1\n","The prediction for image 12821_idx5_x2201_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x2201_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x2201_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x2201_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x2201_y1351_class1.png is: 1\n","The prediction for image 12821_idx5_x2251_y1151_class1.png is: 1\n","The prediction for image 12821_idx5_x2251_y1201_class1.png is: 1\n","The prediction for image 12821_idx5_x2251_y1251_class1.png is: 1\n","The prediction for image 12821_idx5_x2251_y1301_class1.png is: 1\n","The prediction for image 12821_idx5_x951_y1701_class1.png is: 1\n","The prediction for image 12821_idx5_x951_y1751_class1.png is: 1\n"]}],"source":["# PREDICCIONES\n","\n","import os\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","# Define la transformación que necesitas aplicar en las imágenes\n","# Para hacerla compatible con tu modelo\n","transform = transforms.Compose([\n","    transforms.Resize((50,50)),      # Redimensionar a 50x50\n","    transforms.ToTensor(),           # Convertir a tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizar como se hizo durante el entrenamiento\n","])\n","\n","# Define la ruta de tu directorio\n","directory = \"C:\\\\Users\\\\alvaroldg\\\\Desktop\\\\IDC_regular_ps50_idx5\\\\12821\\\\1\"\n","\n","# Obtiene una lista de todos los archivos en el directorio\n","image_files = os.listdir(directory)\n","\n","# Itera sobre cada archivo de imagen\n","for image_file in image_files:\n","    # Abre la imagen usando PIL\n","    image_pred = Image.open(os.path.join(directory, image_file))\n","\n","    # Aplica las transformaciones a la imagen\n","    image_p_tensor = transform(image_pred)\n","\n","    # Añade una dimensión extra en la posición 0\n","    image_p_tensor = image_p_tensor.unsqueeze(0)\n","\n","    # Asegúrate de que el modelo está en modo de evaluación\n","    model.eval()\n","\n","    # Realiza la predicción\n","    with torch.no_grad():\n","        output_p = model(image_p_tensor)\n","        prediction = (output_p >= 0.5).squeeze().long()\n","\n","    # Imprime la predicción\n","    print(f\"The prediction for image {image_file} is: {prediction.item()}\")\n"]},{"cell_type":"code","execution_count":null,"id":"8503b407","metadata":{"id":"8503b407","outputId":"8b48e892-39fd-4464-b131-a7ed5f40196b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 10 neurons\n","CREAR MODELO 10\n","Loaded checkpoint C:\\Users\\alvaroldg\\Desktop\\TFM_Checkpoints\\checkpoint_0020_10.pth\n"]},{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\", \"conv4.weight\", \"conv4.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". ","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4868\\3178923725.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_ckpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_neurons'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2041\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   2042\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   2043\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\", \"conv4.weight\", \"conv4.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". "]}],"source":["# Import necessary libraries\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import ParameterGrid\n","\n","# Convert lists to tensors\n","imagenes = torch.stack([torch.Tensor(i) for i in imagenes])\n","etiquetas = torch.tensor(listado_arrays_etiquetas)\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(imagenes, etiquetas, test_size=0.2, random_state=42)\n","\n","# Define custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# Create train and test datasets\n","train_dataset = CustomDataset(X_train, y_train)\n","test_dataset = CustomDataset(X_test, y_test)\n","\n","# Define the model architecture\n","class Model(nn.Module):\n","    def __init__(self, num_neurons):\n","        super(Model, self).__init__()\n","        self.conv1 = nn.Conv2d(3, num_neurons, 3)\n","        self.conv2 = nn.Conv2d(num_neurons, num_neurons*2, 3)\n","        self.conv3 = nn.Conv2d(num_neurons*2, num_neurons*4, 3)\n","        self.conv4 = nn.Conv2d(num_neurons*4, num_neurons*8, 3)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(num_neurons*8, num_neurons)  # Asume que después de cuatro convoluciones y max pooling, la dimensión es num_neurons * 8 * 3 * 3\n","        self.fc2 = nn.Linear(num_neurons, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = self.pool(F.relu(self.conv4(x)))\n","        x = x.view(-1, self.num_flat_features(x))\n","        x = F.relu(self.fc1(x))\n","        x = self.sigmoid(self.fc2(x))\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","\n","# Define the function to create the model\n","def crear_modelo(num_neurons):\n","    print(\"CREAR MODELO\", num_neurons)\n","    model = Model(num_neurons)\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters())\n","    return model, criterion, optimizer\n","\n","# Define the checkpoint directory\n","checkpoint_dir = 'C:\\\\Users\\\\alvaroldg\\\\Desktop\\\\TFM_Checkpoints'\n","\n","# Create checkpoint directory if it doesn't exist\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","def save_ckpt(model, optimizer, epoch, num_neurons, checkpoint_dir):\n","    save_dir = os.path.join(checkpoint_dir, \"checkpoint_{:04d}_{}.pth\".format(epoch, num_neurons))\n","\n","    cp = {\n","        'model': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","        'epoch': epoch,\n","    }\n","\n","    torch.save(cp, save_dir)\n","    print(\"Saved checkpoint into {}\".format(save_dir))\n","\n","\n","def load_ckpt(checkpoint_dir, num_neurons):\n","    last_epoch = 0\n","    checkpoint_file = ''\n","\n","    for file in os.listdir(checkpoint_dir):\n","        if file.endswith('{}.pth'.format(num_neurons)):\n","            epoch_number = int(file.split('_')[1])\n","            if epoch_number > last_epoch:\n","                last_epoch = epoch_number\n","                checkpoint_file = file\n","\n","    if last_epoch == 0:\n","        return None, 0\n","    else:\n","        print(\"Loaded checkpoint\", os.path.join(checkpoint_dir, checkpoint_file))\n","        checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoint_file))\n","        return checkpoint, checkpoint['epoch']\n","\n","# Define the grid of parameters\n","param_grid = {\n","    'num_neurons': [10, 20, 30],\n","}\n","\n","# Perform grid search\n","best_accuracy = 0.0\n","best_params = None\n","\n","for params in ParameterGrid(param_grid):\n","    print(\"Using {} neurons\".format(params['num_neurons']))\n","    model, criterion, optimizer = crear_modelo(params['num_neurons'])\n","    checkpoint, start_epoch = load_ckpt(checkpoint_dir, params['num_neurons'])\n","    if checkpoint is not None:\n","        model.load_state_dict(checkpoint['model'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n","\n","    for epoch in range(start_epoch, start_epoch + 10):\n","        model.train()\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            inputs = inputs.permute((0,3,1,2))\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels.float().unsqueeze(1))\n","            loss.backward()\n","            optimizer.step()\n","\n","        save_ckpt(model, optimizer, epoch + 1, params['num_neurons'], checkpoint_dir) # Save the checkpoint\n","\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in test_loader:\n","                inputs = inputs.permute((0,3,1,2))\n","                outputs = model(inputs)\n","                predicted = (outputs >= 0.5).squeeze().long()\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        accuracy = correct / total\n","        print(\"Epoch: {}, Accuracy: {}\".format(epoch + 1, accuracy))\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_params = params\n","\n","print(\"Best Accuracy:\", best_accuracy)\n","print(\"Best Parameters:\", best_params)"]},{"cell_type":"code","execution_count":null,"id":"00414b61","metadata":{"id":"00414b61"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}